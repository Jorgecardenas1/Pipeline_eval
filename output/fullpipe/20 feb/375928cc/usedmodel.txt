batch:GAN Training
fitness:0.010091953813471164
Predicted:tensor([[ 0.0730,  0.0444,  0.0102,  0.0194,  0.0492,  0.0520,  0.0397,  0.0298,
          0.0041, -0.0174, -0.0014,  0.0225,  0.0212,  0.0104,  0.0079, -0.0041,
         -0.0097, -0.0078, -0.0095, -0.0125, -0.0219, -0.0171,  0.0247,  0.0544,
          0.0354,  0.0185,  0.0443,  0.0751,  0.0502,  0.0330,  0.0631,  0.1082,
          0.1352,  0.0983,  0.0710,  0.0718,  0.0477,  0.0181,  0.0428,  0.0641,
          0.0658,  0.0611,  0.0327,  0.0011,  0.0204,  0.0644,  0.0670,  0.0225,
         -0.0042,  0.0234,  0.0306,  0.0302,  0.0313,  0.0609,  0.0932,  0.0772,
          0.0333,  0.0322,  0.0666,  0.0681,  0.0500,  0.0600,  0.0586,  0.0575,
          0.0449,  0.0546,  0.0649,  0.0485,  0.0504,  0.0524,  0.0461,  0.0659,
          0.0921,  0.0929,  0.0705,  0.0505,  0.0456,  0.0489,  0.0582,  0.0773,
          0.1004,  0.1106,  0.1013,  0.0822,  0.0721,  0.0708,  0.0776,  0.0729,
          0.0522,  0.0486,  0.0534,  0.0353,  0.0205,  0.0312,  0.0509,  0.0593,
          0.0524,  0.0367,  0.0224,  0.0129]], device='cuda:0',
       grad_fn=<TanhBackward0>)
latent:[-0.9999  0.9936  0.9811 -0.911   0.7466  0.9984  0.91    0.9907  0.984
 -0.9977 -0.9961 -0.8249 -0.9102 -0.1174  0.5169  0.9551 -0.9896 -0.7416
 -0.9997 -0.6226 -0.9942  0.7244  0.9185  0.9986  0.8513 -0.7746 -0.8262
 -0.9863 -0.8895 -0.9542 -0.8476 -0.7526 -0.751  -0.9231 -0.9949 -0.6276
 -0.5579  0.9974 -0.9981  0.4362  0.9691 -0.9951 -0.5636 -0.9753  0.9947
  0.7803 -0.8995  1.     -0.9934 -0.6608 -0.9974  0.9943 -0.6338 -0.9874
  0.9294  0.9369 -0.764   0.7418  0.9538 -0.9922  0.9733  0.9879  0.8413
 -0.9979 -0.9964 -0.9982  0.96    0.7332  0.865  -0.9725 -0.9864 -0.9985
 -0.9737  0.9643  0.8654 -0.7052  0.9943  0.933  -0.8038 -1.      0.9689
 -0.8995  0.9884 -0.9999 -0.6443  0.9324 -0.9268 -0.9818 -0.9991 -0.9544
  0.9462 -0.8524 -0.9575  0.9685 -0.8817 -0.7434 -0.538  -0.9986 -0.9636
 -0.6395 -0.9786  0.4119 -0.8283 -0.948  -0.9924 -0.8261 -0.677   0.9715
  0.9223 -0.5233 -0.9977 -0.9146  0.9353 -0.9565 -0.9989 -0.3534  0.1758
 -0.5261 -0.9943  1.     -0.9977  0.8278 -0.4083  0.9068 -0.9397  0.7365
 -0.6267 -0.9829 -0.9725 -0.9327 -0.9956 -0.9999 -0.9971 -0.9975  0.962
 -0.9356 -0.9945  0.9999  0.8235  0.9949  0.8434 -0.8911 -0.8689 -0.7774
  0.9226  0.7782 -0.974  -0.5886 -0.9901  0.9235 -0.558   0.9896 -0.9695
 -0.5668  0.9837 -0.6814 -0.8744 -0.6909 -0.7528 -0.7951  1.     -0.0029
 -0.9967 -0.9379 -0.9553  0.9309  0.1871 -0.9823 -0.9537 -0.8266 -0.9922
  0.8774 -0.7017 -0.9976 -1.     -0.9993 -0.9948 -0.6555  0.9894  0.525
 -0.9399  0.9576  0.7748  0.5575  0.7856  0.9732  0.4104 -0.512   0.9434
  0.5996  0.9475  0.8625  0.9672 -0.9675 -0.9043 -0.9804 -0.2601  0.9973
  0.9438 -0.7862  0.8236  0.971  -0.9283 -0.9993  0.4729 -0.9937 -0.8359
  0.9894 -0.8074  0.9245 -0.9924  0.9989 -0.9973 -0.6045  0.9892 -0.9972
  0.8765  0.9734 -0.9926 -0.9466 -0.9129 -0.9887  0.9745  0.9984  0.4011
 -0.5668 -0.9994  0.0862  0.9927 -0.7261 -0.9988  0.6711 -0.6934 -0.5369
  0.9914 -0.6337 -0.8624 -0.8949  0.5072  0.0427  0.8252  0.9655 -0.9405
  0.8826 -0.9924  0.9466 -0.8117  0.9999 -0.9758 -0.9855 -0.8456  0.3786
 -1.     -0.9266 -0.9972  0.9653 -0.9709  0.9479  0.9984 -0.8932  0.997
 -0.7565  0.9957  0.0112  0.8251 -0.9972  0.9622 -0.9985  0.9919 -0.9998
 -0.9583 -0.1708 -0.9992 -0.9624 -0.9896  0.969   0.5697 -0.8761 -0.5236
 -0.1325 -0.0539 -0.9855 -0.9026  0.9991 -0.9093  0.6244 -0.5645 -0.8901
 -0.9826  0.9868  0.9769 -0.9628 -0.9866  0.9997 -0.9526 -0.8272 -0.8292
 -0.9679 -0.6598  0.9714 -0.6427  0.93   -1.     -0.9999  0.9912 -0.7648
 -0.8379  0.8755 -0.9989 -0.7645  0.6254  0.8967 -0.9999 -0.9507 -0.907
 -0.2232  0.9085 -0.7208 -0.4852 -0.9675 -0.9887  0.989  -0.7479 -0.9971
 -0.9614 -0.855  -0.8616  0.9981 -0.5842  0.9949  0.4867  0.9499 -0.9997
  0.891   0.979  -0.3713  0.6922 -0.9314 -0.9979  0.998   0.9952  0.9938
 -0.9267  0.9825 -0.9656 -0.9999 -0.7507  0.5688 -0.9542 -0.9983 -0.8936
  0.4872 -0.9897 -0.9081 -0.9945 -0.8785 -0.8041  0.9996  0.9935 -0.9805
 -0.9999 -0.7352  0.9496  0.9103  0.2898  0.241  -0.1424 -0.9954  0.9933
 -0.9998  0.9994 -0.9537  0.8009 -0.9985  0.9516 -0.2766 -0.1952 -0.9977
  0.9406 -0.9569 -0.9948  0.9971  0.82   -0.9576  0.99    0.9872  0.9075
 -0.8187 -0.6418 -0.974  -0.9567 -0.9923 -0.773  -0.8944 -0.9852 -0.46
  0.774   0.5906  0.9868 -0.9198]
Truth:[[7.50300000e+01 7.50600000e+01 7.50900000e+01 7.51200000e+01
  7.51500000e+01 7.51800000e+01 7.52100000e+01 7.52400000e+01
  7.52700000e+01 7.53000000e+01 7.53300000e+01 7.53600000e+01
  7.53900000e+01 7.54200000e+01 7.54500000e+01 7.54800000e+01
  7.55100000e+01 7.55400000e+01 7.55700000e+01 7.56000000e+01
  7.56300000e+01 7.56600000e+01 7.56900000e+01 7.57200000e+01
  7.57500000e+01 7.57800000e+01 7.58100000e+01 7.58400000e+01
  7.58700000e+01 7.59000000e+01 7.59300000e+01 7.59600000e+01
  7.59900000e+01 7.60200000e+01 7.60500000e+01 7.60800000e+01
  7.61100000e+01 7.61400000e+01 7.61700000e+01 7.62000000e+01
  7.62300000e+01 7.62600000e+01 7.62900000e+01 7.63200000e+01
  7.63500000e+01 7.63800000e+01 7.64100000e+01 7.64400000e+01
  7.64700000e+01 7.65000000e+01 7.65300000e+01 7.65600000e+01
  7.65900000e+01 7.66200000e+01 7.66500000e+01 7.66800000e+01
  7.67100000e+01 7.67400000e+01 7.67700000e+01 7.68000000e+01
  7.68300000e+01 7.68600000e+01 7.68900000e+01 7.69200000e+01
  7.69500000e+01 7.69800000e+01 7.70100000e+01 7.70400000e+01
  7.70700000e+01 7.71000000e+01 7.71300000e+01 7.71600000e+01
  7.71900000e+01 7.72200000e+01 7.72500000e+01 7.72800000e+01
  7.73100000e+01 7.73400000e+01 7.73700000e+01 7.74000000e+01
  7.74300000e+01 7.74600000e+01 7.74900000e+01 7.75200000e+01
  7.75500000e+01 7.75800000e+01 7.76100000e+01 7.76400000e+01
  7.76700000e+01 7.77000000e+01 7.77300000e+01 7.77600000e+01
  7.77900000e+01 7.78200000e+01 7.78500000e+01 7.78800000e+01
  7.79100000e+01 7.79400000e+01 7.79700000e+01 7.80000000e+01]
 [2.01335504e-02 2.01341551e-02 2.01436630e-02 2.01646975e-02
  2.02003509e-02 2.02543075e-02 2.03310092e-02 2.04358767e-02
  2.05756116e-02 2.07586139e-02 2.09955684e-02 2.13002825e-02
  2.16909079e-02 2.21917615e-02 2.28361057e-02 2.36705177e-02
  2.47619751e-02 2.62097789e-02 2.81664928e-02 3.08766119e-02
  3.47523806e-02 4.05336347e-02 4.96562471e-02 6.52017638e-02
  9.47142517e-02 1.60074130e-01 3.35353283e-01 7.25859317e-01
  5.39068947e-01 2.17310329e-01 1.05517192e-01 6.26582569e-02
  4.27330260e-02 3.20570635e-02 2.57232651e-02 2.16688734e-02
  1.89156995e-02 1.69550855e-02 1.55027452e-02 1.43903427e-02
  1.35133812e-02 1.28043950e-02 1.22183594e-02 1.17243633e-02
  1.13006599e-02 1.09316212e-02 1.06058043e-02 1.03146924e-02
  1.00518543e-02 9.81237195e-03 9.59244338e-03 9.38910297e-03
  9.20002192e-03 9.02336469e-03 8.85768548e-03 8.70185436e-03
  8.55500643e-03 8.41651002e-03 8.28595289e-03 8.16314768e-03
  8.04816186e-03 7.94138310e-03 7.84364231e-03 7.75643770e-03
  7.68234985e-03 7.62584237e-03 7.59490337e-03 7.60469184e-03
  7.68653013e-03 7.91336865e-03 8.48692540e-03 1.01278597e-02
  1.65257951e-02 3.33167263e-02 1.51722347e-02 9.35126554e-03
  7.74360575e-03 7.08492389e-03 6.73483668e-03 6.51292615e-03
  6.35379261e-03 6.22931884e-03 6.12584684e-03 6.03610461e-03
  5.95593218e-03 5.88280766e-03 5.81512668e-03 5.75182581e-03
  5.69217457e-03 5.63565508e-03 5.58188963e-03 5.53059556e-03
  5.48155628e-03 5.43460220e-03 5.38959782e-03 5.34643291e-03
  5.30501627e-03 5.26527126e-03 5.22713263e-03 5.19054412e-03]]
initial conditioning:tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.5080, 5.0052]])
final conditioning:tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.5080, 5.0660]])
initial conditioning:5.031
final conditioning:tensor(5.0660)
