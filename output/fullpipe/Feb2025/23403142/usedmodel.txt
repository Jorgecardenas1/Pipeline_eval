batch:GAN Training
fitness:0.058460235899277777
Predicte:tensor([[-0.0468, -0.0150,  0.0152,  0.0075, -0.0038, -0.0208, -0.0199, -0.0168,
          0.0061,  0.0243,  0.0564,  0.0886,  0.1045,  0.1308,  0.1399,  0.1480,
          0.1607,  0.1563,  0.1632,  0.1673,  0.1649,  0.1497,  0.1268,  0.1154,
          0.1128,  0.1461,  0.1629,  0.1600,  0.1317,  0.0936,  0.0491,  0.0465,
          0.0333,  0.0377,  0.0282,  0.0365,  0.0354,  0.0445,  0.0526,  0.0463,
          0.0625,  0.1008,  0.1629,  0.0939, -0.0223,  0.0221,  0.0523,  0.0948,
          0.1222,  0.0763,  0.0636,  0.0892,  0.1138,  0.1157,  0.1272,  0.1685,
          0.1776,  0.1548,  0.1244,  0.1336,  0.1438,  0.1766,  0.1698,  0.1403,
          0.1221,  0.1038,  0.0973,  0.1179,  0.1281,  0.1310,  0.1506,  0.1787,
          0.1985,  0.2064,  0.2131,  0.2304,  0.2454,  0.2932,  0.3010,  0.3324,
          0.3460,  0.3389,  0.3043,  0.3051,  0.3088,  0.3084,  0.2900,  0.2544,
          0.2170,  0.1871,  0.1490,  0.1455,  0.1177,  0.0968,  0.0948,  0.0908,
          0.0813,  0.0601,  0.0671,  0.0498]], device='cuda:0',
       grad_fn=<TanhBackward0>)
latent:[ 0.766  0.769 -0.     0.177  0.878  0.053  0.306  1.     0.063  0.983
  0.662  0.127  0.091  0.058  0.83   0.986  0.912  0.79   0.208  0.917
  0.018  0.118  0.981  0.029  0.169  0.093  0.007  0.869  0.587  0.933
  0.619  0.937  0.957  0.961  0.718  0.711  0.347  0.942  0.976  0.73
  0.994  0.299  0.98   0.012  0.215  0.444  0.993  0.213  0.015  0.08
 -0.     0.004  0.996  0.763  0.095  0.004  0.72   0.011  0.791  0.999
  0.212  0.868  0.026  0.413  0.977  0.158  0.012  0.96   0.888  0.258
  0.834  0.27   0.876  0.839  0.01   0.031  0.479  0.314 -0.     0.732
  0.527  0.957  0.186  0.021  0.976  0.933  0.995  0.12   0.927  0.491
  0.906  0.135  0.283  0.231  0.134  0.858  0.816  0.673  0.428  0.826
  0.223  0.057  0.768  0.965  0.975  0.957  0.422  0.907 -0.     0.852
  0.892  0.869  0.719  0.035  0.967  0.931  0.085  0.004  0.031  0.334
  0.867  0.967  0.934  0.581  0.145  0.959  0.845  0.177  0.907  0.556
  0.076  0.111  0.213  0.987  0.333  0.103  0.1    0.852  0.337  0.056
  0.68   0.043  0.88   0.499  0.708  0.991  0.106  0.23   0.039  0.02
  0.01   0.137  0.052  0.003  0.987  0.992  0.453  0.006  0.142  0.133
  0.047  0.041  0.762  0.155  0.505  0.862  0.968  0.103  0.999  0.967
  0.995  0.712  0.084  0.983  0.58   0.117  0.02   0.92   0.763 -0.
  0.825  0.996  0.036  0.979  0.209  0.987  0.995  0.039  0.007  0.849
  0.987  0.883 -0.     0.024  0.781  0.369  0.244  0.171  0.014  0.708
  0.163  0.858  0.67   0.95   0.985  0.026  0.972  0.087  0.778  0.138
  0.093  0.43   0.154  0.904  0.422  0.981  0.948  0.914  0.399  0.026
  0.926  0.06   0.945  0.75   0.093  0.092  0.505  0.003  0.962  0.194
  0.757  0.001  0.939  0.327  0.071  0.023  0.999  0.852  0.444  0.009
  0.878  0.371  0.842  0.457  0.017  0.521  0.858  0.998  0.554  0.107
  0.909  0.118  0.018  0.017  0.457  0.713  0.963  0.056  0.009  0.152
  0.11   0.054  0.942  0.253  0.651  0.996  0.01   0.82   0.343  0.662
  0.18   0.418  0.017  0.013  0.926  0.804  1.     0.979  0.884  0.135
  0.015  0.975  0.419  0.001  1.     0.079  0.097  0.097  0.054  0.212
  0.568  0.115  0.821  0.965  1.     0.644  0.017  0.054  0.838  0.849
  0.747  0.856  0.631  0.802  0.81   0.836  0.603  0.075  0.975  0.98
  0.986  0.831  0.475  0.008  0.361  0.119  0.068  0.119  0.147  0.059
  0.79   0.312  0.095  0.37   0.036  0.752  0.05   0.429  0.84   0.724
  0.997  0.7    0.834  0.946  0.335  0.08   0.986  0.841  0.434  0.186
  0.248  0.456  0.632  0.702  0.955  0.044  0.093  0.938  0.742  0.056
  0.191  0.263  0.858  0.115  0.881  0.942  0.974  0.079  0.82   0.978
  0.779  0.994  0.095  0.098  0.803  0.746  0.961  0.734  0.858  0.15
  0.965  0.039  0.045  0.251  0.98   0.232  0.025  0.931  0.737  0.865
  0.035  0.974  0.072  0.031  0.499  0.865  0.121  0.948  0.225  0.543
  0.116  0.93   0.439  0.887  0.937  0.821  0.707  0.016  0.843  0.153]
Truth:[[7.50300000e+01 7.50600000e+01 7.50900000e+01 7.51200000e+01
  7.51500000e+01 7.51800000e+01 7.52100000e+01 7.52400000e+01
  7.52700000e+01 7.53000000e+01 7.53300000e+01 7.53600000e+01
  7.53900000e+01 7.54200000e+01 7.54500000e+01 7.54800000e+01
  7.55100000e+01 7.55400000e+01 7.55700000e+01 7.56000000e+01
  7.56300000e+01 7.56600000e+01 7.56900000e+01 7.57200000e+01
  7.57500000e+01 7.57800000e+01 7.58100000e+01 7.58400000e+01
  7.58700000e+01 7.59000000e+01 7.59300000e+01 7.59600000e+01
  7.59900000e+01 7.60200000e+01 7.60500000e+01 7.60800000e+01
  7.61100000e+01 7.61400000e+01 7.61700000e+01 7.62000000e+01
  7.62300000e+01 7.62600000e+01 7.62900000e+01 7.63200000e+01
  7.63500000e+01 7.63800000e+01 7.64100000e+01 7.64400000e+01
  7.64700000e+01 7.65000000e+01 7.65300000e+01 7.65600000e+01
  7.65900000e+01 7.66200000e+01 7.66500000e+01 7.66800000e+01
  7.67100000e+01 7.67400000e+01 7.67700000e+01 7.68000000e+01
  7.68300000e+01 7.68600000e+01 7.68900000e+01 7.69200000e+01
  7.69500000e+01 7.69800000e+01 7.70100000e+01 7.70400000e+01
  7.70700000e+01 7.71000000e+01 7.71300000e+01 7.71600000e+01
  7.71900000e+01 7.72200000e+01 7.72500000e+01 7.72800000e+01
  7.73100000e+01 7.73400000e+01 7.73700000e+01 7.74000000e+01
  7.74300000e+01 7.74600000e+01 7.74900000e+01 7.75200000e+01
  7.75500000e+01 7.75800000e+01 7.76100000e+01 7.76400000e+01
  7.76700000e+01 7.77000000e+01 7.77300000e+01 7.77600000e+01
  7.77900000e+01 7.78200000e+01 7.78500000e+01 7.78800000e+01
  7.79100000e+01 7.79400000e+01 7.79700000e+01 7.80000000e+01]
 [1.94539328e-02 2.10218092e-02 2.27744986e-02 2.47402945e-02
  2.69533589e-02 2.94551553e-02 3.22963046e-02 3.55390077e-02
  3.92602345e-02 4.35559625e-02 4.85468607e-02 5.43859873e-02
  6.12693178e-02 6.94502834e-02 7.92600360e-02 9.11359243e-02
  1.05661749e-01 1.23624770e-01 1.46095945e-01 1.74540511e-01
  2.10962415e-01 2.58068768e-01 3.19383969e-01 3.99088002e-01
  5.00994021e-01 6.25484334e-01 7.63140762e-01 8.87309564e-01
  9.57487901e-01 9.45256097e-01 8.60238768e-01 7.39348406e-01
  6.16455058e-01 5.08909437e-01 4.21199885e-01 3.51795064e-01
  2.97381114e-01 2.54659506e-01 2.20895179e-01 1.93972371e-01
  1.72298907e-01 1.54686801e-01 1.40248791e-01 1.28318339e-01
  1.18390542e-01 1.10079314e-01 1.03086494e-01 9.71795489e-02
  9.21753758e-02 8.79284513e-02 8.43220901e-02 8.12619350e-02
  7.86710643e-02 7.64862818e-02 7.46552772e-02 7.31344331e-02
  7.18871175e-02 7.08823424e-02 7.00937015e-02 6.94985226e-02
  6.90771856e-02 6.88125701e-02 6.86896052e-02 6.86948997e-02
  6.88164372e-02 6.90433244e-02 6.93655815e-02 6.97739698e-02
  7.02598489e-02 7.08150604e-02 7.14318359e-02 7.21027241e-02
  7.28205389e-02 7.35783242e-02 7.43693354e-02 7.51870375e-02
  7.60251176e-02 7.68775112e-02 7.77384425e-02 7.86024758e-02
  7.94645781e-02 8.03201904e-02 8.11653069e-02 8.19965596e-02
  8.28113067e-02 8.36077226e-02 8.43848879e-02 8.51428768e-02
  8.58828411e-02 8.66070888e-02 8.73191554e-02 8.80238689e-02
  8.87274053e-02 8.94373366e-02 9.01626692e-02 9.09138744e-02
  9.17029104e-02 9.25432361e-02 9.34498170e-02 9.44391237e-02]]
conditioning:tensor([[0.0000, 0.0000, 0.0000, 0.1913, 0.0972, 0.9767]])
conditioning:('splitcross_02_freq_reflect_v2_ddefee0a-e894-11ef-96c2-047c16a08772_0-61_75-78.png',)
