batch:GAN Training
fitness:0.00919683370706089
Predicte:tensor([[ 0.0172,  0.0062, -0.0105, -0.0215, -0.0269, -0.0243, -0.0085, -0.0087,
          0.0143,  0.0385,  0.0440,  0.0447,  0.0527,  0.0741,  0.0818,  0.0683,
          0.0625,  0.0408,  0.0169,  0.0058,  0.0233,  0.0587,  0.0645,  0.0644,
          0.0632,  0.0620,  0.0753,  0.0558,  0.0460,  0.0266,  0.0169,  0.0261,
         -0.0069,  0.0030,  0.0228,  0.0316,  0.0451,  0.0293,  0.0306,  0.0109,
          0.0329,  0.0589,  0.0682,  0.0588,  0.0411,  0.0486,  0.0430,  0.0600,
          0.0582,  0.0042,  0.0157,  0.0201,  0.0035, -0.0183, -0.0114,  0.0449,
          0.0505,  0.0404,  0.0323,  0.0468,  0.0335,  0.0486,  0.0336,  0.0211,
          0.0234,  0.0272,  0.0231, -0.0063, -0.0455, -0.0644, -0.0601, -0.0247,
          0.0393,  0.0598,  0.0797,  0.0910,  0.0904,  0.0748,  0.0606,  0.0709,
          0.0888,  0.0911,  0.1024,  0.1129,  0.1209,  0.1148,  0.0923,  0.0693,
          0.0673,  0.0493,  0.0453,  0.0475,  0.0475,  0.0356,  0.0423,  0.0360,
          0.0236,  0.0178,  0.0220, -0.0038]], device='cuda:0',
       grad_fn=<TanhBackward0>)
latent:[ 0.941  0.965  0.751  0.326  0.479  0.684  0.411  0.531  0.996  0.928
  0.978  0.543  0.621  0.391  0.465  0.504  0.969  0.035  0.904  0.178
  0.874  0.646  0.606  0.348  0.881  0.015  0.007  0.909  0.695  0.971
  0.961  0.707  0.879  0.144  0.068  0.022  0.6    0.409  0.081  0.914
  0.78   0.265  0.05   0.731  0.951  0.427  0.497  0.857  0.998  0.012
  0.272  0.088  0.864  0.256  0.04   0.563  0.73   0.239  0.106  0.023
  0.732  0.174  0.625  0.921  0.873  0.145  0.519  0.072  0.135  0.503
  0.572  0.012  0.878  0.603  0.261  0.447  0.843  0.578  0.809  0.266
  0.513  0.98   0.073  0.619  0.087  0.888  0.9    0.477  0.653  0.334
  0.437  0.298  0.29   0.894  0.475  0.58   0.011  0.958  0.477  0.031
  0.024  0.617  0.731  0.398  0.072  0.939  0.973  0.368  0.286  0.363
  0.559  0.985  0.606  0.38   0.009  0.321  0.153  0.347  0.088  0.466
  0.02   0.97   0.805  0.42   0.348  0.92   0.814 -0.     0.898  0.135
  0.241  0.047  0.117  0.459  0.365  0.052  0.17   0.537  0.185  0.157
  0.831  0.073  0.003  0.69   0.74   0.011  0.017  0.034  0.576  0.077
  0.312  0.744  0.828  0.951  0.119  0.568  0.275  0.956  0.83   0.91
  0.09   0.457  0.16   0.567  0.481  0.011  0.199  0.504  0.777  0.954
  0.974  0.902  0.059  0.761  0.148  0.969  0.111  0.206  0.256  0.811
  0.566  0.976  0.112  0.463  0.03   0.513  0.096  0.417  0.441  0.713
  0.087  0.436  0.538  0.261  0.935  0.057  0.425  0.429  0.254  0.527
  0.172  0.02   0.377  0.253  0.651  0.98   0.374  0.293  0.087  0.008
  0.704  0.186  0.188  0.609  0.604  0.599  0.873  0.659  0.071  0.385
  0.518  0.962  0.541  0.558  0.67   0.01   0.776  0.989  0.752  0.983
  0.019  0.595  0.36   0.355  0.222  0.774  0.897  0.992  0.995  0.985
  0.58   0.243  0.244  0.511  0.784  0.341  0.003  0.033  0.025  0.397
  0.508  0.705  0.559  0.983  0.628  0.822  0.576  0.816  0.062  0.738
  0.833  0.475  0.001  0.468  0.241  0.094  0.533  0.41   0.659  0.298
  0.418  0.392  0.043  0.906  0.063  0.012  0.318  0.494  0.156  0.076
  0.502  0.535  0.529  0.988  0.394  0.035  0.063  0.18   0.493  0.26
  0.044  0.639  0.244  0.063  0.964  0.349  0.322  0.773  0.614  0.306
  0.638  0.06   0.859  0.381  0.029  0.229  0.242  0.894  0.069  0.79
  0.693  0.956  0.477  0.908  0.955  0.081  0.072  0.543  0.921  0.965
  0.659  0.308  0.016  0.985  0.994  0.338  0.048  0.003  0.505  0.013
  0.889  0.437  0.278  0.973  0.109  0.486  0.119  0.605  0.081  0.037
  0.513  0.612  0.132  0.005  0.855  0.065  0.348  0.4    0.869  0.001
  0.043  0.858  0.997  0.673  0.028  0.982  0.383  0.014  0.982  0.934
  0.967  0.017  0.036  0.539  0.68   0.586  0.246  0.981  0.998  0.722
  0.456  0.255  0.719  0.88   0.028  0.271  0.064  0.955  0.525  0.231
  0.095  0.51   0.13   0.407  0.096  0.065  0.016  0.428  0.808  0.399
  0.505  0.079  0.034  0.995  0.033  0.068  0.043  0.927  0.609  0.058]
Truth:[[7.50300000e+01 7.50600000e+01 7.50900000e+01 7.51200000e+01
  7.51500000e+01 7.51800000e+01 7.52100000e+01 7.52400000e+01
  7.52700000e+01 7.53000000e+01 7.53300000e+01 7.53600000e+01
  7.53900000e+01 7.54200000e+01 7.54500000e+01 7.54800000e+01
  7.55100000e+01 7.55400000e+01 7.55700000e+01 7.56000000e+01
  7.56300000e+01 7.56600000e+01 7.56900000e+01 7.57200000e+01
  7.57500000e+01 7.57800000e+01 7.58100000e+01 7.58400000e+01
  7.58700000e+01 7.59000000e+01 7.59300000e+01 7.59600000e+01
  7.59900000e+01 7.60200000e+01 7.60500000e+01 7.60800000e+01
  7.61100000e+01 7.61400000e+01 7.61700000e+01 7.62000000e+01
  7.62300000e+01 7.62600000e+01 7.62900000e+01 7.63200000e+01
  7.63500000e+01 7.63800000e+01 7.64100000e+01 7.64400000e+01
  7.64700000e+01 7.65000000e+01 7.65300000e+01 7.65600000e+01
  7.65900000e+01 7.66200000e+01 7.66500000e+01 7.66800000e+01
  7.67100000e+01 7.67400000e+01 7.67700000e+01 7.68000000e+01
  7.68300000e+01 7.68600000e+01 7.68900000e+01 7.69200000e+01
  7.69500000e+01 7.69800000e+01 7.70100000e+01 7.70400000e+01
  7.70700000e+01 7.71000000e+01 7.71300000e+01 7.71600000e+01
  7.71900000e+01 7.72200000e+01 7.72500000e+01 7.72800000e+01
  7.73100000e+01 7.73400000e+01 7.73700000e+01 7.74000000e+01
  7.74300000e+01 7.74600000e+01 7.74900000e+01 7.75200000e+01
  7.75500000e+01 7.75800000e+01 7.76100000e+01 7.76400000e+01
  7.76700000e+01 7.77000000e+01 7.77300000e+01 7.77600000e+01
  7.77900000e+01 7.78200000e+01 7.78500000e+01 7.78800000e+01
  7.79100000e+01 7.79400000e+01 7.79700000e+01 7.80000000e+01]
 [1.11371378e-02 1.12312339e-02 1.13184538e-02 1.13997065e-02
  1.14758894e-02 1.15479040e-02 1.16166724e-02 1.16831533e-02
  1.17483600e-02 1.18133813e-02 1.18794046e-02 1.19477453e-02
  1.20198820e-02 1.20975019e-02 1.21825575e-02 1.22773423e-02
  1.23845889e-02 1.25076018e-02 1.26504371e-02 1.28181516e-02
  1.30171521e-02 1.32556969e-02 1.35446296e-02 1.38984801e-02
  1.43371620e-02 1.48886779e-02 1.55936009e-02 1.65128739e-02
  1.77422923e-02 1.94420868e-02 2.19077477e-02 2.58008923e-02
  3.35830592e-02 6.12673419e-02 6.57148869e-02 1.05418233e-01
  2.43787463e-01 7.49516872e-01 4.06865684e-01 1.24829661e-01
  5.68741133e-02 3.33820523e-02 2.29358731e-02 1.74904970e-02
  1.43285276e-02 1.23429535e-02 1.10190918e-02 1.00930948e-02
  9.41916464e-03 8.91180701e-03 8.51841697e-03 8.20531303e-03
  7.95019735e-03 7.73788264e-03 7.55776846e-03 7.40229692e-03
  7.26597806e-03 7.14475789e-03 7.03559881e-03 6.93619487e-03
  6.84477495e-03 6.75996432e-03 6.68068567e-03 6.60608748e-03
  6.53549135e-03 6.46835294e-03 6.40423261e-03 6.34277315e-03
  6.28368272e-03 6.22672168e-03 6.17169231e-03 6.11843081e-03
  6.06680086e-03 6.01668862e-03 5.96799862e-03 5.92065049e-03
  5.87457632e-03 5.82971848e-03 5.78602780e-03 5.74346216e-03
  5.70198520e-03 5.66156533e-03 5.62217487e-03 5.58378926e-03
  5.54638650e-03 5.50994659e-03 5.47445105e-03 5.43988251e-03
  5.40622439e-03 5.37346054e-03 5.34157499e-03 5.31055168e-03
  5.28037423e-03 5.25102574e-03 5.22248858e-03 5.19474425e-03
  5.16777315e-03 5.14155450e-03 5.11606611e-03 5.09128434e-03]]
conditioning:tensor([[0.0000, 0.1953, 0.0000, 0.0000, 0.0992, 0.9757]])
conditioning:('cross_01_freq_reflect_v2_46cdb140-c380-11ef-80cc-047c16a08772_0-275_75-78.png',)
